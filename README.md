# ëª¨ë“ ê±¸ ìë™ìœ¼ë¡œ!

## ğŸ‘¨ğŸ»â€ğŸ’»í”„ë¡œì íŠ¸ ê°„ë‹¨ìš”ì•½

LG í—¬ë¡œtv ìœ ì € ì…‹í†±ë°•ìŠ¤ ë¡œê·¸ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ VOD ê°œì¸í™” ì¶”ì²œ

- ì„œë¹„ìŠ¤ ì‹œì—°ì˜ìƒ
    
    https://drive.google.com/file/d/1xiMbqaHBmcul35GPBYMKBQfg4Onz8bTH/view?usp=drive_link
    

---

## ğŸ™‹ğŸ»ë‚˜ëŠ” ë¬´ì—‡ì„ í–ˆëŠ”ê°€?

- í”„ë¡œì íŠ¸ ì¼ì§€
    
    [ì œëª© ì—†ëŠ” ë°ì´í„°ë² ì´ìŠ¤](https://www.notion.so/f0bcf5beca1149ff86a9b6a8251ca2b3?pvs=21)
    

1. AWS í™˜ê²½ì—ì„œ Cloud Architectureë¥¼ ì„¤ê³„ ë° êµ¬ì¶•í•˜ì˜€ìŠµë‹ˆë‹¤.
    - Architecture
        
        ![vod architecture.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/df3849f7-f233-4fe8-88e6-7e7528b6fb56/c4693218-d971-4102-b82c-83f737adb39f/vod_architecture.png)
        
    - ë°°í¬ëœ ì‚¬ì§„
        - ECS(Django ì„œë²„)
            
            ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/df3849f7-f233-4fe8-88e6-7e7528b6fb56/5095b604-7273-46fd-b9ba-020405fc3b7e/Untitled.png)
            
        - S3(React ì›¹)
            
            ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/df3849f7-f233-4fe8-88e6-7e7528b6fb56/90c53227-1805-4349-a0b4-76ade9821854/Untitled.png)
            
        - RDS(MySQL)
            
            ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/df3849f7-f233-4fe8-88e6-7e7528b6fb56/72aa6709-e625-40be-bd23-b313cba78edb/Untitled.png)
            
        - EC2(Airflow)
            
            ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/df3849f7-f233-4fe8-88e6-7e7528b6fb56/15d41d7c-ce7a-4ff9-b910-2a656a5df5f1/Untitled.png)
            
        
    
2. Airflowë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„° ì „ì²˜ë¦¬ ë° ëª¨ë¸ ì¬í•™ìŠµì„ í•˜ê³  ëª¨ë¸ ì—…ë°ì´íŠ¸ í›„ ìƒˆë¡œìš´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëŒ€ì‹œë³´ë“œì— ê¸°ë¡í•˜ëŠ” ê³¼ì •ì„ ì „ì²´ ìë™í™” í•˜ì˜€ìŠµë‹ˆë‹¤.
    - Workflow
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/df3849f7-f233-4fe8-88e6-7e7528b6fb56/fb874add-ef39-484e-a862-3916c5111d2f/Untitled.png)
        
    - DAG ì½”ë“œ
        
        ```python
        #ìµœì¢… dag ì½”ë“œ
        import csv
        import json
        import pandas as pd
        import numpy as np
        import logging
        import pendulum
        import pickle
        from datetime import datetime, timedelta
        from tempfile import NamedTemporaryFile
        from airflow import DAG
        from airflow.operators.python import PythonOperator
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook
        from airflow.hooks.mysql_hook import MySqlHook
        from sklearn.model_selection import train_test_split
        from tqdm import tqdm
        from surprise import SVD, Reader, Dataset, accuracy
        from lightfm import LightFM, cross_validation
        from lightfm.data import Dataset
        from sklearn.metrics.pairwise import cosine_similarity
        from scipy.sparse import coo_matrix
        
        # ëª¨ë¸ í´ë˜ìŠ¤
        class LightFM_Model:
                def __init__(self, data, vod_info, user_info, param_loss='logistic', param_components=10, param_epochs=10):
                    self.vod_info = vod_info
                    self.user_info = user_info
                    self.train, self.test = self.split_evaluate(data)
                    self.train_interactions, self.train_weights = self.dataset(self.train)
                    self.score_matrix = self.create_score_matrix(data)
                    self.score_matrix_evaluate = self.create_score_matrix(self.train)
                    ####
                    self.loss = param_loss
                    self.components = param_components
                    self.epoch = param_epochs
                    self.precision, self.recall, self.map, self.mar, self.test_diversity, self.user_metrics = self.evaluate(self.train_interactions, self.train_weights, self.test)
                    self.model = self.train_model(data)
                    self.all_diversity = self.evaluate_all(self.model)
        
                def split_evaluate(self, data):
                    #### ìˆ˜ì • ì™„ë£Œ
                    train, test = train_test_split(data[data['ì‹œì²­ì—¬ë¶€'].notnull()][['subsr_id', 'program_id', 'rating']], test_size=0.25, random_state=0)
                    train = data[['subsr_id', 'program_id', 'rating', 'ì‹œì²­ì—¬ë¶€']].copy()
                    train.loc[test.index, 'rating'] = np.nan
                    return train, test
        
                #### ìˆ˜ì • ì™„ë£Œ
                def create_score_matrix(self, data):
                    df = data.copy()
                    df.loc[df[(df['ì‹œì²­ì—¬ë¶€']==1) & df['rating'].notnull()].index, 'score_matrix'] = 1
                    score_matrix = df.pivot(columns='program_id', index='subsr_id', values='score_matrix')
                    return score_matrix
        
                def dataset(self, train):
                    dataset = Dataset()
                    dataset.fit(users = train['subsr_id'].sort_values().unique(),
                                items = train['program_id'].sort_values().unique())
                    num_users, num_vods = dataset.interactions_shape()
                    ####
                    train['interaction'] = train['rating'].apply(lambda x: 0 if np.isnan(x) else 1)
                    train_dropna = train.dropna()
                    train_interactions = coo_matrix((train_dropna['interaction'], (train_dropna['subsr_id'], train_dropna['program_id'])), shape=(train['subsr_id'].max() + 1, train['program_id'].max() +1))
                    train_weights = coo_matrix((train_dropna['rating'], (train_dropna['subsr_id'], train_dropna['program_id'])), shape=(train['subsr_id'].max() + 1, train['program_id'].max() +1))
                    #(train_interactions, train_weights) = dataset.build_interactions(train[['subsr_id', 'program_id', 'rating']].dropna().values)
                    return train_interactions, train_weights
        
                def fit(self, fitting_interactions, fitting_weights):
                    model = LightFM(random_state=0, loss=self.loss, no_components=self.components)
                    model.fit(interactions=fitting_interactions, sample_weight=fitting_weights, verbose=1, epochs=self.epoch)
                    return model
        
                def predict(self, subsr_id, program_id, model):
                    pred = model.predict([subsr_id], [program_id])
                    return pred
        
                def recommend(self, subsr_id, score_matrix, model, N):
                    # ì•ˆ ë³¸ vod ì¶”ì¶œ
                    user_rated = score_matrix.loc[subsr_id].dropna().index.tolist()
                    user_unrated = score_matrix.columns.drop(user_rated).tolist()
                    # ì•ˆë³¸ vodì— ëŒ€í•´ì„œ ì˜ˆì¸¡í•˜ê¸°
                    predictions = model.predict(int(subsr_id), user_unrated)
                    result = pd.DataFrame({'program_id':user_unrated, 'pred_rating':predictions})
                    # predê°’ì— ë”°ë¥¸ ì •ë ¬í•´ì„œ ê²°ê³¼ ë„ìš°ê¸°
                    top_N = result.sort_values(by='pred_rating', ascending=False)[:N]
                    return top_N
        
                @staticmethod
                def precision_recall_at_k(target, prediction):
                    num_hit = len(set(prediction).intersection(set(target)))
                    precision = float(num_hit) / len(prediction) if len(prediction) > 0 else 0.0
                    recall = float(num_hit) / len(target) if len(target) > 0 else 0.0
                    return precision, recall
        
                @staticmethod
                def map_at_k(target, prediction, k=10):
                    num_hits = 0
                    precision_at_k = 0.0
                    for i, p in enumerate(prediction[:k]):
                        if p in target:
                            num_hits += 1
                            precision_at_k += num_hits / (i + 1)
                    if not target.any():
                        return 0.0
                    return precision_at_k / min(k, len(target))
        
                @staticmethod
                def mar_at_k(target, prediction, k=10):
                    num_hits = 0
                    recall_at_k = 0.0
                    for i, p in enumerate(prediction[:k]):
                        if p in target:
                            num_hits += 1
                            recall_at_k += num_hits / len(target)
                    if not target.any():
                        return 0.0
                    return recall_at_k / min(k, len(target))
        
                def evaluate(self, train_interactions, train_weights, test, N=10):
                    evaluate_model = self.fit(train_interactions, train_weights)
                    result = pd.DataFrame()
                    precisions = []
                    recalls = []
                    map_values = []
                    mar_values = []
                    user_metrics = []
        
                    for idx, user in enumerate(tqdm(test['subsr_id'].unique())):
                        targets = test[test['subsr_id']==user]['program_id'].values
                        predictions = self.recommend(user, self.score_matrix_evaluate, evaluate_model, N)['program_id'].values
                        precision, recall = self.precision_recall_at_k(targets, predictions)
                        map_at_k_value = self.map_at_k(targets, predictions)
                        mar_at_k_value = self.mar_at_k(targets, predictions)
                        precisions.append(precision)
                        recalls.append(recall)
                        map_values.append(map_at_k_value)
                        mar_values.append(mar_at_k_value)
                        user_metrics.append({'subsr_id': user, 'precision': precision, 'recall': recall, 'map_at_k': map_at_k_value, 'mar_at_k': mar_at_k_value})
        
                        result.loc[idx, 'subsr_id'] = user
                        for rank in range(len(predictions)):
                            result.loc[idx, f'vod_{rank}'] = predictions[rank]
        
                    list_sim = cosine_similarity(result.iloc[:, 1:])
                    list_similarity = np.sum(list_sim - np.eye(len(result))) / (len(result) * (len(result) - 1))
        
                    return np.mean(precisions), np.mean(recalls), np.mean(map_values), np.mean(mar_values), 1-list_similarity, pd.DataFrame(user_metrics)
        
                def evaluate_all(self, model, N=10):
                    result = pd.DataFrame()
                    for idx, user in enumerate(tqdm(self.user_info['subsr_id'])):
                        predictions = self.recommend(user, self.score_matrix, model, N)['program_id'].values
                        result.loc[idx, 'subsr_id'] = user
                        for rank in range(len(predictions)):
                            result.loc[idx, f'vod_{rank}'] = predictions[rank]
        
                    list_sim = cosine_similarity(result.iloc[:, 1:])
                    list_similarity = np.sum(list_sim - np.eye(len(result))) / (len(result) * (len(result) - 1))
                    return 1 - list_similarity
        
                def train_model(self, data):
                    # ìµœì¢… í•™ìŠµ ë°ì´í„°ì…‹ ë§Œë“¤ê¸°
                    dataset = Dataset()
                    dataset.fit(users = data['subsr_id'].sort_values().unique(),
                                items = data['program_id'].sort_values().unique())
                    num_users, num_vods = dataset.interactions_shape()
                    ####
                    data['interaction'] = data['rating'].apply(lambda x: 0 if np.isnan(x) else 1)
                    data_dropna = data.dropna()
                    train_interactions = coo_matrix((data_dropna['interaction'], (data_dropna['subsr_id'], data_dropna['program_id'])))
                    train_weights = coo_matrix((data_dropna['rating'], (data_dropna['subsr_id'], data_dropna['program_id'])))
                    #(train_interactions, train_weights) = dataset.build_interactions(data[['subsr_id', 'program_id', 'rating']].dropna().values)
                    # fitting
                    model = self.fit(train_interactions, train_weights)
                    return model
        
        # ê¸°ë³¸ ì„¤ì •
        default_args = {
            'owner': 'admin',
            'retries': 5,
            'retry_delay': timedelta(minutes=10)
        }
        
        local_tz = pendulum.timezone("Asia/Seoul")
        
        # ì£¼ì°¨ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° ë° ìƒˆë¡œìš´ ì£¼ì°¨ì •ë³´ ì €ì¥
        def week_info_s3(**context):
            s3_hook = S3Hook(aws_conn_id='aws_default')
        
            # S3ì— íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            file_exists = s3_hook.check_for_key('week_info/week_info.json', 'hello00.net-airflow')
        
            #íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ìƒˆ íŒŒì¼ ìƒì„±, ì¡´ì¬í•  ê²½ìš°ì—ëŠ” ì½ì–´ì˜¤ê¸°
            if not file_exists:
                logging.info("*****íŒŒì¼ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìƒˆë¡œìƒì„±*****")
                new_json = {"columns": ["week_info"],"values": [40]}
                updated_json_data = json.dumps(new_json, indent=2)
                current_week=new_json["values"][0]
                s3_hook.load_string(updated_json_data, 'week_info/week_info.json', 'hello00.net-airflow', replace=True)
        
            else:
                logging.info("*****íŒŒì¼ ì¡´ì¬. ê¸°ì¡´ íŒŒì¼ ì½ì–´ì˜´*****")
                existing_data = s3_hook.read_key('week_info/week_info.json', 'hello00.net-airflow')
                existing_json = json.loads(existing_data)
                #ì´ì „ ì£¼ì°¨ ì •ë³´ ì½ê¸°
                current_week=existing_json["values"][0] + 1
        
                #ê¸°ì¡´ ì£¼ì°¨ ì •ë³´ ì—…ë°ì´íŠ¸
                existing_json["values"] = [current_week]
                updated_json = json.dumps(existing_json, indent=2)
                s3_hook.load_string(updated_json, 'week_info/week_info.json', 'hello00.net-airflow', replace=True)
            
            logging.info("*****í˜„ì¬ week ì •ë³´ ë°›ì•„ì˜´*****")
        
            logging.info(current_week)
            context["task_instance"].xcom_push(key="current_week", value=current_week)
        
        # MySQL ë°ì´í„°ë² ì´ìŠ¤ë¡œë¶€í„° ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        def mysql_hook(**context):
            
            current_week = context["task_instance"].xcom_pull(task_ids="week_info", key="current_week")
            logging.info("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°")
            
            hook = MySqlHook.get_hook(conn_id="mysql-01")  # ë¯¸ë¦¬ ì •ì˜í•œ MySQL connection ì ìš©
            connection = hook.get_conn()  # connection í•˜ê¸°
            cursor = connection.cursor()  # cursor ê°ì²´ ë§Œë“¤ê¸°
            cursor.execute("use vod_rec")  # SQL ë¬¸ ìˆ˜í–‰
        
            users = pd.read_sql('select * from userinfo', connection)
            vods = pd.read_sql(f'select * from vods_sumut where week(log_dt)<={current_week}', connection)
            conts = pd.read_sql(f'select * from contlog where week(log_dt)<={current_week}', connection)
            program_info_all = pd.read_sql('select * from vodinfo', connection)
        
            logging.info(users)
            logging.info(vods)
            logging.info(conts)
            logging.info(program_info_all)
        
            context["task_instance"].xcom_push(key="users", value=users)
            context["task_instance"].xcom_push(key="vods", value=vods)
            context["task_instance"].xcom_push(key="conts", value=conts)
            context["task_instance"].xcom_push(key="program_info_all", value=program_info_all)
            context["task_instance"].xcom_push(key="program_info_all", value=program_info_all)
        
            connection.close()
        
        # ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
        def data_preprocessing(**context):
            logging.info("ë°ì´í„° ì „ì²˜ë¦¬")
        
            # mysql_hook í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í–ˆë˜ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
            users = context["task_instance"].xcom_pull(task_ids="mysql_hook", key="users")
            vods = context["task_instance"].xcom_pull(task_ids="mysql_hook", key="vods")
            conts = context["task_instance"].xcom_pull(task_ids="mysql_hook", key="conts")
            program_info_all = context["task_instance"].xcom_pull(task_ids="mysql_hook", key="program_info_all")
        
            # ì „ì²˜ë¦¬ ì‹œì‘
            # e_bool == 0 ì¸ ë°ì´í„°ë§Œ ë½‘ê¸°
            vod_log = vods[vods['e_bool']==0][['subsr_id', 'program_id', 'program_name', 'episode_num', 'log_dt', 'use_tms', 'disp_rtm_sec', 'count_watch', 'month']]
            cont_log = conts[conts['e_bool']==0][['subsr_id', 'program_id', 'program_name', 'episode_num', 'log_dt', 'month']]
            vod_info = program_info_all[program_info_all['e_bool']==0][['program_id','program_name', 'ct_cl', 'program_genre', 'release_date', 'age_limit']]
            user_info = users.copy()
        
            vod_log['use_tms_ratio'] = vod_log['use_tms'] / vod_log['disp_rtm_sec']
            vod_log['log_dt'] = pd.to_datetime(vod_log['log_dt'])
            cont_log['log_dt'] = pd.to_datetime(cont_log['log_dt'])
            cont_log['recency'] = (cont_log['log_dt'].max() - cont_log['log_dt']).dt.days # ìµœê·¼ì„±
        
            log = pd.concat([vod_log[['subsr_id', 'program_id']], cont_log[['subsr_id', 'program_id']]]).drop_duplicates().reset_index(drop=True)
            log = log.merge(cont_log.groupby(['subsr_id', 'program_id'])[['program_name']].count().reset_index().rename(columns={'program_name':'click_cnt'}), how='left')
            log = log.merge(cont_log.groupby(['subsr_id', 'program_id'])[['recency']].min().reset_index().rename(columns={'recency':'click_recency'}), how='left')
        
            # (subsr_id, program_id) ìŒì— ëŒ€í•´ í•˜ë‚˜ì˜ í‰ì ë§Œ ë‚¨ê²¨ì•¼ í•¨
            # => use_tms_ratio ëŠ” ìµœëŒ€ê°’ìœ¼ë¡œ ë‚¨ê¸°ê³ , ì‹œì²­ countë„ í•´ì„œ ì¶”ê°€í•œë‹¤.
            log = log.merge(vod_log.groupby(['subsr_id', 'program_id'])[['use_tms_ratio']].max().reset_index().rename(columns={'use_tms_ratio':'watch_tms_max'}), how='left')
            log = log.merge(vod_log.groupby(['subsr_id', 'program_id'])[['use_tms_ratio']].count().reset_index().rename(columns={'use_tms_ratio':'watch_cnt'}), how='left')
        
            replace_value = log['click_cnt'].quantile(0.95) # 95% ë¡œ ìµœëŒ€ê°’ ê³ ì •
            log.loc[log[log['click_cnt'] > replace_value].index, 'click_cnt'] = replace_value
        
            log['click_recency'] = log['click_recency'].max() - log['click_recency']
        
            # ì‹œì²­ê¸°ë¡ë§Œ ì‚¬ìš©
            replace_value = log['watch_cnt'].quantile(0.95)  # 95% ë¡œ ìµœëŒ€ê°’ ê³ ì •
            log.loc[log[log['watch_cnt'] > replace_value].index, 'watch_cnt'] = replace_value
        
            log_rating = log.copy()
            log_rating['rating'] = log_rating['watch_tms_max']
            log_rating.loc[log_rating[log_rating['rating']>0].index, 'ì‹œì²­ì—¬ë¶€'] = 1
            rating_df = log_rating[['subsr_id', 'program_id', 'rating', 'ì‹œì²­ì—¬ë¶€']]
            vod_info['program_id'] = vod_info['program_id'].astype('int')
            rating_df['program_id'] = rating_df['program_id'].astype('int')
            vod_info = vod_info.merge(log_rating.groupby(['program_id', 'subsr_id'])[['click_cnt']].count().groupby('program_id').count().reset_index(), how='left')
        
            max_subsr_id = rating_df['subsr_id'].max()
            user_info = user_info[user_info['subsr_id'] < max_subsr_id]
            
            logging.info(vod_info)
            logging.info(rating_df)
        
            context["task_instance"].xcom_push(key="rating_df", value=rating_df)
            context["task_instance"].xcom_push(key="vod_info", value=vod_info)
            context["task_instance"].xcom_push(key="user_info", value=user_info)
        
        # ëª¨ë¸ ì ìš© ë° ì„±ëŠ¥ì¶”ì¶œ & pickleíŒŒì¼ s3ì— ì €ì¥
        def model_running(**context):
            rating_df = context["task_instance"].xcom_pull(task_ids="data_preprocessing", key="rating_df")
            vod_info = context["task_instance"].xcom_pull(task_ids="data_preprocessing", key="vod_info")
            user_info = context["task_instance"].xcom_pull(task_ids="data_preprocessing", key="user_info")
        
            
            # ëª¨ë¸ í´ë˜ìŠ¤ ê°ì²´ ìƒì„±í•˜ê³ , ì„±ëŠ¥ ì¶œë ¥í•˜ëŠ” ì½”ë“œ
            lfm_model = LightFM_Model(rating_df, vod_info, user_info, 'warp', 30, 10)
        
            Precision = round(lfm_model.precision, 5)
            Recall = round(lfm_model.recall, 5)
            MAP = round(lfm_model.map, 5)
            MAR = round(lfm_model.mar, 5)
            test_Diversity = round(lfm_model.test_diversity, 5)
            all_Diversity = round(lfm_model.all_diversity, 5)
        
            context["task_instance"].xcom_push(key="Precision", value=Precision)
            context["task_instance"].xcom_push(key="Recall", value=Recall)
            context["task_instance"].xcom_push(key="MAP", value=MAP)
            context["task_instance"].xcom_push(key="MAR", value=MAR)
            context["task_instance"].xcom_push(key="test_Diversity", value=test_Diversity)
            context["task_instance"].xcom_push(key="all_Diversity", value=all_Diversity)
        
            logging.info("í”¼í´íŒŒì¼ êµì²´")
            with open('model_lightfm.pkl', 'wb') as f:
                pickle.dump(lfm_model, f)
            logging.info("í”¼í´ìƒì„± ì™„ë£Œ")
            s3_hook = S3Hook(aws_conn_id='aws_default')
            s3_hook.load_file('model_lightfm.pkl', 'model_lightfm.pkl', 'hello00.net-model', replace=True)
        
        # ì„±ëŠ¥ S3ì— ì ì¬
        def convert_to_json(**context):
            s3_hook = S3Hook(aws_conn_id='aws_default')
            current_week = context["task_instance"].xcom_pull(task_ids="week_info", key="current_week")
            current_year = datetime.now().year
            # ì£¼ì°¨ì˜ ì²«ì§¸ë‚ ê³¼ ë§ˆì§€ë§‰ë‚ ì„ ê³„ì‚°
            # first_day_of_week = datetime.strptime(f'{current_year}-W{current_week-1}-0', "%Y-W%W-%w").date()
            # last_day_of_week = first_day_of_week + timedelta(days=6)
            # current_week = f"{first_day_of_week.strftime('%Y.%m.%d')} ~ {last_day_of_week.strftime('%Y.%m.%d')}"
            logging.info(f"{current_week} ë‚ ì§œ ì‚½ì…")
        
            # Metrics to append
            metrics = [
                {"name": "Precision", "value": context["task_instance"].xcom_pull(task_ids="model_running", key="Precision")},
                {"name": "Recall", "value": context["task_instance"].xcom_pull(task_ids="model_running", key="Recall")},
                {"name": "MAP", "value": context["task_instance"].xcom_pull(task_ids="model_running", key="MAP")},
                {"name": "MAR", "value": context["task_instance"].xcom_pull(task_ids="model_running", key="MAR")},
                {"name": "test_Diversity", "value": context["task_instance"].xcom_pull(task_ids="model_running", key="test_Diversity")},
                {"name": "all_Diversity", "value": context["task_instance"].xcom_pull(task_ids="model_running", key="all_Diversity")},
            ]
        
            for metric in metrics:
                metric_name = metric["name"]
                metric_value = metric["value"]
        
                # S3ì— íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                file_exists = s3_hook.check_for_key(f'model_accuracy/{metric_name.lower()}.json', 'hello00.net-airflow')
                
                start_date = '2023.08.01'
                # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ìƒˆ íŒŒì¼ ìƒì„±, ì¡´ì¬í•  ê²½ìš°ì—ëŠ” ê¸°ì¡´ íŒŒì¼ì— ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
                if not file_exists:
                    new_metrics = {"columns": [[f'{metric_name}({start_date})']], "values": [[metric_value]]}
                    updated_json_data = json.dumps(new_metrics, indent=2)
                else:
                    existing_data = s3_hook.read_key(f'model_accuracy/{metric_name.lower()}.json', 'hello00.net-airflow')
                    existing_metrics = json.loads(existing_data)
        
                    # ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
                    existing_metrics["columns"]=[[f'{metric_name}({start_date})']]
                    existing_metrics["values"].append([metric_value])
        
                    # Jsonìœ¼ë¡œ ë³€í™˜
                    updated_json_data = json.dumps(existing_metrics, indent=2)
        
                # S3ì— ë‹¤ì‹œ ì €ì¥
                s3_hook.load_string(updated_json_data, f'model_accuracy/{metric_name.lower()}.json', 'hello00.net-airflow', replace=True)
        
        # DAG ì„¤ì •
        with DAG(
            dag_id="VOD_Recommendation",
            default_args=default_args,
            start_date=datetime(2023, 12, 22, tzinfo=local_tz),
            schedule_interval='@once' 
        ) as dag:
            week_info = PythonOperator(
            task_id="week_info",
            python_callable=week_info_s3
            )
            database = PythonOperator(
                task_id="mysql_hook",
                python_callable=mysql_hook
            )
            data_preprocess = PythonOperator(
            task_id="data_preprocessing",
            python_callable=data_preprocessing
            )
            model_apply = PythonOperator(
                task_id="model_running",
                python_callable=model_running
            )
            dashboard_data_save = PythonOperator(
                task_id="convert_to_json",
                python_callable=convert_to_json
            )
        
            week_info >> database >> data_preprocess >> model_apply >> dashboard_data_save
        ```
        
    - ëŒ€ì‹œë³´ë“œ í˜ì´ì§€ ì˜ìƒ ë° ì‚¬ì§„
        
        https://drive.google.com/file/d/1zfFu3EqKOi-Z4CwCBTEc7cArpBDdamJV/view?usp=drive_link
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/df3849f7-f233-4fe8-88e6-7e7528b6fb56/a9b34618-c61f-441f-91e0-6e4ba8941f75/Untitled.png)
        
    - Airflow ì‹œì—°ì˜ìƒ
        
        [airflow ì‘ë™ì˜ìƒ.mp4](https://prod-files-secure.s3.us-west-2.amazonaws.com/df3849f7-f233-4fe8-88e6-7e7528b6fb56/82320ac5-7b71-48fa-b685-5add204d0b7f/airflow_%EC%9E%91%EB%8F%99%EC%98%81%EC%83%81.mp4)
        

---

## ğŸ“–ë‚˜ëŠ” ë¬´ì—‡ì„ ë°°ì› ëŠ”ê°€?

1. ì´ì „ì— ì‹¤ìŠµìœ¼ë¡œë§Œ êµ¬ì¶•í•´ë³´ì•˜ë˜ ECS, S3, RDSë¥¼ ì‹¤ì „ìœ¼ë¡œ ì‚¬ìš©í•´ë³´ì•˜ë˜ ê²ƒì— í° ì˜ì˜ê°€ ìˆë‹¤. ê°ê°ì˜ AWS ì„œë¹„ìŠ¤ë“¤ì„ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°í•˜ë©´ì„œ **ë„¤íŠ¸ì›Œí¬ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆì—ˆë‹¤.**
2. í”„ë¡œì íŠ¸ ì¤‘ IAM ì—‘ì„¸ìŠ¤ í‚¤ê°€ ë…¸ì¶œë˜ë©´ì„œ ê³„ì •ì´ í•´í‚¹ë‹¹í•˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•˜ì˜€ë‹¤. ì´ë¥¼í†µí•´ **ê°œì¸ ë³´ì•ˆê³¼ ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆì˜ ì¤‘ìš”ì„±ì„ ì•Œê²Œ ë˜ì—ˆë‹¤.**
3. Airflowë¥¼ ì²˜ìŒ ì‚¬ìš©í•´ë³´ì•˜ëŠ”ë°, ê¸°ì¡´ì—ëŠ” **Airflowê°€ ë‹¨ì§€ ë¡œê·¸ ìˆ˜ì§‘ê¸° ì •ë„ì˜ ì—­í• ì„ í•  ì¤„ ì•Œì•˜ë˜ ë‚˜ì˜ ìƒê°ì„ ë°”ê¾¸ì–´ì£¼ì—ˆë‹¤.** ë°ì´í„° í•¸ë“¤ë§, ë¨¸ì‹ ëŸ¬ë‹ ë“± pythonìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ” ëŒ€ë¶€ë¶„ì˜ ê²ƒë“¤ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ë§¤ìš° í¸ë¦¬í•œ ë„êµ¬ë¼ ìƒê°í–ˆë‹¤.

---

## â•ì•ìœ¼ë¡œ ë¬´ì—‡ì„ ë” ë³´ì™„í•  ìˆ˜ ìˆëŠ”ê°€?

1. **ë¹„ìš©ì  ì¸¡ë©´**ì—ì„œ ECSë³´ë‹¤ EC2ì— ì„œë²„ë¥¼ êµ¬ì¶•í•˜ëŠ”ê²ƒì´ ì¢‹ì„ ìˆ˜ ìˆë‹¤. í•„ìš”ì— ë”°ë¼ EC2ì— ëª¨ë‘ ìƒì„±í•˜ëŠ” ë°©ë²•ë„ ê³ ë ¤í•´ë³´ì.
2. EC2ì— Airflowë¥¼ ì„¤ì¹˜í–ˆì—ˆëŠ”ë°, AWSì˜ **MWAA ì„œë¹„ìŠ¤ë„ ì‚¬ìš©í•  ì¤„ ì•Œì•„ì•¼í•œë‹¤**. ë‹¤ë§Œ ì—­ì‹œ ë¹„ìš©ì  ì¸¡ë©´ì—ì„œ ì¢‹ì§€ ì•Šë‹¤. MWAAë¥¼ ì´í‹€ì •ë„ êµ¬ë™í–ˆì„ë•Œ 50ë¶ˆ ì •ë„ê°€ ì²­êµ¬ë˜ì—ˆë‹¤.
3. **ë³´ì•ˆì„ ê°•í™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤**. private subnetê³¼ vpcë“±ì„ ì¡°ì •í•´ì„œ ì™¸ë¶€ë¡œë¶€í„°ì˜ ë³´ì•ˆ ìœ„í˜‘ì„ ë§‰ì„ ìˆ˜ ìˆë‹¤.
